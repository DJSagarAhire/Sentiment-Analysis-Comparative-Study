{
 "metadata": {
  "name": ""
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "##Additional Tests\n",
      "\n",
      "This notebook contains additional tests performed on the best-performing methods on the respective features. They are:\n",
      "\n",
      "<ol>\n",
      "    <li>SVM for Unigram Presence</li>\n",
      "    <li>SVM for Unigram Counts</li>\n",
      "    <li>SVM for Bigram Presence</li>\n",
      "    <li>Naive Bayes for Bigram Counts</li>\n",
      "</ol>\n",
      "\n",
      "The additiona tests include:\n",
      "\n",
      "<ol>\n",
      "    <li>Effect of Corpus Size on Performance</li>\n",
      "    <li>Effect of Removing Stop-Words</li>\n",
      "    <li>TF-IDF Weighting</li>\n",
      "</ol>"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "###Imports and Function Setup"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from sklearn import datasets"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 56
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%pylab inline"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Populating the interactive namespace from numpy and matplotlib\n"
       ]
      }
     ],
     "prompt_number": 57
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Set the path where the data resides\n",
      "datapath = \"../data/movies-english/\""
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 58
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Load train set\n",
      "train_set_location = \"{0}train\".format(datapath)\n",
      "train_set = datasets.load_files(train_set_location)\n",
      "\n",
      "# Load test set\n",
      "test_set_location = \"{0}test\".format(datapath)\n",
      "test_set = datasets.load_files(test_set_location)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 59
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 60
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Function setup:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def classify(classifier):\n",
      "    classifier.fit(train_X, train_y)\n",
      "    predicted_y = classifier.predict(test_X)\n",
      "    \n",
      "    return predicted_y\n",
      "\n",
      "def select_classify(classifier, k):\n",
      "    \n",
      "    # First select\n",
      "    from sklearn.feature_selection import chi2, SelectKBest\n",
      "    selector = SelectKBest(chi2, k)\n",
      "    unigram_seltrain_X = selector.fit_transform(train_X, train_y)\n",
      "    \n",
      "    classifier.fit(unigram_seltrain_X, train_y)\n",
      "    predicted_y = classifier.predict(selector.transform(test_X))\n",
      "    \n",
      "    return predicted_y"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 61
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def accuracy():\n",
      "    from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
      "\n",
      "    acc = accuracy_score(test_y, predict_y) * 100\n",
      "    print(\"Accuracy: {0}%\".format(acc))\n",
      "    print(classification_report(test_y, predict_y, target_names=test_set.target_names))\n",
      "    \n",
      "    return acc"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 62
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "###Removing Stop Words\n",
      "\n",
      "In this test, we remove the stop words in the documents and observe how the accuracy is affected. The default `movies-english` corpus is used for this (and all subsequent) tests."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "####Unigram Presence with SVM"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Setting up the Vectorizer:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Create the vectorizer\n",
      "unigram_presence_vectorizer = CountVectorizer(binary=True, stop_words='english')\n",
      "\n",
      "# Fit training data and transform appropriately\n",
      "train_X = unigram_presence_vectorizer.fit_transform(train_set.data)\n",
      "train_y = train_set.target\n",
      "\n",
      "# Transform test data according to learnt training vocabulary\n",
      "test_X = unigram_presence_vectorizer.transform(test_set.data)\n",
      "test_y = test_set.target"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 63
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print(\"The list of {0} stop words is:\\n{1}\".format(len(unigram_presence_vectorizer.get_stop_words()), unigram_presence_vectorizer.get_stop_words()))\n",
      "\n",
      "print(\"Length of feature vector: {0}\".format(train_X.shape[1]))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "The list of 318 stop words is:\n",
        "frozenset({'move', 'mine', 'namely', 'its', 'almost', 'him', 'thence', 'part', 'by', 'somehow', 'your', 'might', 'her', 'name', 'formerly', 'be', 'thereafter', 'somewhere', 'afterwards', 'alone', 'along', 'whereby', 'seemed', 'but', 'inc', 'and', 'found', 'both', 'each', 'many', 'meanwhile', 'hereupon', 'else', 'over', 'below', 'any', 'un', 'less', 'cant', 'otherwise', 'hence', 'again', 'for', 'between', 'five', 'other', 'while', 'their', 'amoungst', 'whose', 'you', 'then', 'nine', 'will', 'whence', 'throughout', 'get', 'interest', 'per', 'always', 'must', 'whom', 'whenever', 'back', 'seem', 'very', 'or', 'itself', 'go', 'cry', 'anything', 'due', 'until', 'out', 'beforehand', 'on', 'nowhere', 'besides', 'six', 'thru', 'of', 'into', 'ours', 'full', 'ourselves', 'thereupon', 'have', 'how', 'wherein', 'everything', 'what', 'forty', 'sometimes', 'before', 'everywhere', 'latterly', 'whereupon', 'however', 'whole', 'show', 'was', 'de', 'eleven', 'everyone', 'ever', 'further', 'same', 'yours', 'all', 'do', 'bottom', 'the', 'describe', 'also', 'even', 'his', 're', 'above', 'our', 'enough', 'ten', 'seems', 'sometime', 'yet', 'as', 'whereas', 'thin', 'perhaps', 'anywhere', 'at', 'call', 'an', 'am', 'around', 'where', 'is', 'upon', 'hasnt', 'few', 'it', 'among', 'about', 'whoever', 'through', 'please', 'detail', 'eight', 'least', 'after', 'if', 'ie', 'becomes', 'whither', 'already', 'myself', 'in', 'elsewhere', 'they', 'hereby', 'who', 'may', 'two', 'under', 'neither', 'anyway', 'nothing', 'a', 'something', 'onto', 'without', 'whether', 'why', 'system', 'i', 'would', 'thick', 'indeed', 'only', 'should', 'those', 'across', 'beyond', 'twenty', 'us', 'up', 'keep', 'third', 'fifteen', 'once', 'though', 'last', 'never', 'whereafter', 'four', 'these', 'no', 'during', 'either', 'been', 'fire', 'one', 'anyhow', 'from', 'therefore', 'together', 'several', 'off', 'first', 'although', 'within', 'amongst', 'amount', 'own', 'noone', 'etc', 'herself', 'toward', 'co', 'become', 'cannot', 'anyone', 'yourself', 'side', 'moreover', 'with', 'thereby', 'has', 'front', 'this', 'well', 'becoming', 'she', 'than', 'had', 'themselves', 'that', 'mostly', 'are', 'could', 'such', 'beside', 'another', 'con', 'latter', 'against', 'hundred', 'much', 'see', 'often', 'nobody', 'via', 'wherever', 'nevertheless', 'take', 'most', 'not', 'next', 'fify', 'now', 'every', 'nor', 'find', 'therein', 'three', 'sixty', 'others', 'which', 'none', 'made', 'done', 'when', 'towards', 'behind', 'except', 'we', 'he', 'couldnt', 'seeming', 'whatever', 'yourselves', 'sincere', 'ltd', 'serious', 'rather', 'more', 'top', 'herein', 'give', 'being', 'put', 'empty', 'because', 'hereafter', 'hers', 'can', 'bill', 'here', 'too', 'were', 'himself', 'eg', 'them', 'to', 'thus', 'my', 'became', 'some', 'since', 'mill', 'someone', 'fill', 'down', 'former', 'still', 'me', 'twelve', 'so', 'there'})\n",
        "Length of feature vector: 23949\n"
       ]
      }
     ],
     "prompt_number": 64
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from sklearn.svm import LinearSVC\n",
      "\n",
      "unigram_linsvm = LinearSVC()\n",
      "predict_y = classify(unigram_linsvm)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 65
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "unigram_presence_linsvm_accuracy = accuracy()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Accuracy: 79.5%\n",
        "             precision    recall  f1-score   support\n",
        "\n",
        "        neg       0.77      0.84      0.80       500\n",
        "        pos       0.83      0.75      0.78       500\n",
        "\n",
        "avg / total       0.80      0.80      0.79      1000\n",
        "\n"
       ]
      }
     ],
     "prompt_number": 66
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "As a result, for SVM with Unigram Presence, the accuracy remains largely unchanged even after removing the stop words (79.8% vs. 79.5%)."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "####Unigram Counts with SVM"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Setting up the vectorizer:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Create the vectorizer\n",
      "unigram_counts_vectorizer = CountVectorizer(stop_words='english')\n",
      "\n",
      "# Fit training data and transform appropriately\n",
      "train_X = unigram_counts_vectorizer.fit_transform(train_set.data)\n",
      "train_y = train_set.target\n",
      "\n",
      "# Transform test data according to learnt training vocabulary\n",
      "test_X = unigram_counts_vectorizer.transform(test_set.data)\n",
      "test_y = test_set.target"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 67
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print(\"The list of {0} stop words is:\\n{1}\".format(len(unigram_counts_vectorizer.get_stop_words()), unigram_counts_vectorizer.get_stop_words()))\n",
      "\n",
      "print(\"Length of feature vector: {0}\".format(train_X.shape[1]))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "The list of 318 stop words is:\n",
        "frozenset({'move', 'mine', 'namely', 'its', 'almost', 'him', 'thence', 'part', 'by', 'somehow', 'your', 'might', 'her', 'name', 'formerly', 'be', 'thereafter', 'somewhere', 'afterwards', 'alone', 'along', 'whereby', 'seemed', 'but', 'inc', 'and', 'found', 'both', 'each', 'many', 'meanwhile', 'hereupon', 'else', 'over', 'below', 'any', 'un', 'less', 'cant', 'otherwise', 'hence', 'again', 'for', 'between', 'five', 'other', 'while', 'their', 'amoungst', 'whose', 'you', 'then', 'nine', 'will', 'whence', 'throughout', 'get', 'interest', 'per', 'always', 'must', 'whom', 'whenever', 'back', 'seem', 'very', 'or', 'itself', 'go', 'cry', 'anything', 'due', 'until', 'out', 'beforehand', 'on', 'nowhere', 'besides', 'six', 'thru', 'of', 'into', 'ours', 'full', 'ourselves', 'thereupon', 'have', 'how', 'wherein', 'everything', 'what', 'forty', 'sometimes', 'before', 'everywhere', 'latterly', 'whereupon', 'however', 'whole', 'show', 'was', 'de', 'eleven', 'everyone', 'ever', 'further', 'same', 'yours', 'all', 'do', 'bottom', 'the', 'describe', 'also', 'even', 'his', 're', 'above', 'our', 'enough', 'ten', 'seems', 'sometime', 'yet', 'as', 'whereas', 'thin', 'perhaps', 'anywhere', 'at', 'call', 'an', 'am', 'around', 'where', 'is', 'upon', 'hasnt', 'few', 'it', 'among', 'about', 'whoever', 'through', 'please', 'detail', 'eight', 'least', 'after', 'if', 'ie', 'becomes', 'whither', 'already', 'myself', 'in', 'elsewhere', 'they', 'hereby', 'who', 'may', 'two', 'under', 'neither', 'anyway', 'nothing', 'a', 'something', 'onto', 'without', 'whether', 'why', 'system', 'i', 'would', 'thick', 'indeed', 'only', 'should', 'those', 'across', 'beyond', 'twenty', 'us', 'up', 'keep', 'third', 'fifteen', 'once', 'though', 'last', 'never', 'whereafter', 'four', 'these', 'no', 'during', 'either', 'been', 'fire', 'one', 'anyhow', 'from', 'therefore', 'together', 'several', 'off', 'first', 'although', 'within', 'amongst', 'amount', 'own', 'noone', 'etc', 'herself', 'toward', 'co', 'become', 'cannot', 'anyone', 'yourself', 'side', 'moreover', 'with', 'thereby', 'has', 'front', 'this', 'well', 'becoming', 'she', 'than', 'had', 'themselves', 'that', 'mostly', 'are', 'could', 'such', 'beside', 'another', 'con', 'latter', 'against', 'hundred', 'much', 'see', 'often', 'nobody', 'via', 'wherever', 'nevertheless', 'take', 'most', 'not', 'next', 'fify', 'now', 'every', 'nor', 'find', 'therein', 'three', 'sixty', 'others', 'which', 'none', 'made', 'done', 'when', 'towards', 'behind', 'except', 'we', 'he', 'couldnt', 'seeming', 'whatever', 'yourselves', 'sincere', 'ltd', 'serious', 'rather', 'more', 'top', 'herein', 'give', 'being', 'put', 'empty', 'because', 'hereafter', 'hers', 'can', 'bill', 'here', 'too', 'were', 'himself', 'eg', 'them', 'to', 'thus', 'my', 'became', 'some', 'since', 'mill', 'someone', 'fill', 'down', 'former', 'still', 'me', 'twelve', 'so', 'there'})\n",
        "Length of feature vector: 23949\n"
       ]
      }
     ],
     "prompt_number": 68
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from sklearn.svm import LinearSVC\n",
      "\n",
      "unigram_linsvm = LinearSVC()\n",
      "predict_y = classify(unigram_linsvm)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 69
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "unigram_count_linsvm_accuracy = accuracy()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Accuracy: 78.2%\n",
        "             precision    recall  f1-score   support\n",
        "\n",
        "        neg       0.75      0.85      0.80       500\n",
        "        pos       0.83      0.71      0.77       500\n",
        "\n",
        "avg / total       0.79      0.78      0.78      1000\n",
        "\n"
       ]
      }
     ],
     "prompt_number": 70
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The result for Unigram Counts is similar to Unigram Presence. Removal of stop words does not affect the accuracy (78.8% vs. 78.2%). In fact there is a slight *decrease*."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "####Bigram Presence with SVM"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Setting up the vectorizer:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Create the vectorizer\n",
      "bigram_presence_vectorizer = CountVectorizer(binary=True, ngram_range=(2,2), stop_words='english')\n",
      "\n",
      "# Fit training data and transform appropriately\n",
      "train_X = bigram_presence_vectorizer.fit_transform(train_set.data)\n",
      "train_y = train_set.target\n",
      "\n",
      "# Transform test data according to learnt training vocabulary\n",
      "test_X = bigram_presence_vectorizer.transform(test_set.data)\n",
      "test_y = test_set.target"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 71
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print(\"The list of {0} stop words is:\\n{1}\".format(len(bigram_presence_vectorizer.get_stop_words()), bigram_presence_vectorizer.get_stop_words()))\n",
      "\n",
      "print(\"Length of feature vector: {0}\".format(train_X.shape[1]))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "The list of 318 stop words is:\n",
        "frozenset({'move', 'mine', 'namely', 'its', 'almost', 'him', 'thence', 'part', 'by', 'somehow', 'your', 'might', 'her', 'name', 'formerly', 'be', 'thereafter', 'somewhere', 'afterwards', 'alone', 'along', 'whereby', 'seemed', 'but', 'inc', 'and', 'found', 'both', 'each', 'many', 'meanwhile', 'hereupon', 'else', 'over', 'below', 'any', 'un', 'less', 'cant', 'otherwise', 'hence', 'again', 'for', 'between', 'five', 'other', 'while', 'their', 'amoungst', 'whose', 'you', 'then', 'nine', 'will', 'whence', 'throughout', 'get', 'interest', 'per', 'always', 'must', 'whom', 'whenever', 'back', 'seem', 'very', 'or', 'itself', 'go', 'cry', 'anything', 'due', 'until', 'out', 'beforehand', 'on', 'nowhere', 'besides', 'six', 'thru', 'of', 'into', 'ours', 'full', 'ourselves', 'thereupon', 'have', 'how', 'wherein', 'everything', 'what', 'forty', 'sometimes', 'before', 'everywhere', 'latterly', 'whereupon', 'however', 'whole', 'show', 'was', 'de', 'eleven', 'everyone', 'ever', 'further', 'same', 'yours', 'all', 'do', 'bottom', 'the', 'describe', 'also', 'even', 'his', 're', 'above', 'our', 'enough', 'ten', 'seems', 'sometime', 'yet', 'as', 'whereas', 'thin', 'perhaps', 'anywhere', 'at', 'call', 'an', 'am', 'around', 'where', 'is', 'upon', 'hasnt', 'few', 'it', 'among', 'about', 'whoever', 'through', 'please', 'detail', 'eight', 'least', 'after', 'if', 'ie', 'becomes', 'whither', 'already', 'myself', 'in', 'elsewhere', 'they', 'hereby', 'who', 'may', 'two', 'under', 'neither', 'anyway', 'nothing', 'a', 'something', 'onto', 'without', 'whether', 'why', 'system', 'i', 'would', 'thick', 'indeed', 'only', 'should', 'those', 'across', 'beyond', 'twenty', 'us', 'up', 'keep', 'third', 'fifteen', 'once', 'though', 'last', 'never', 'whereafter', 'four', 'these', 'no', 'during', 'either', 'been', 'fire', 'one', 'anyhow', 'from', 'therefore', 'together', 'several', 'off', 'first', 'although', 'within', 'amongst', 'amount', 'own', 'noone', 'etc', 'herself', 'toward', 'co', 'become', 'cannot', 'anyone', 'yourself', 'side', 'moreover', 'with', 'thereby', 'has', 'front', 'this', 'well', 'becoming', 'she', 'than', 'had', 'themselves', 'that', 'mostly', 'are', 'could', 'such', 'beside', 'another', 'con', 'latter', 'against', 'hundred', 'much', 'see', 'often', 'nobody', 'via', 'wherever', 'nevertheless', 'take', 'most', 'not', 'next', 'fify', 'now', 'every', 'nor', 'find', 'therein', 'three', 'sixty', 'others', 'which', 'none', 'made', 'done', 'when', 'towards', 'behind', 'except', 'we', 'he', 'couldnt', 'seeming', 'whatever', 'yourselves', 'sincere', 'ltd', 'serious', 'rather', 'more', 'top', 'herein', 'give', 'being', 'put', 'empty', 'because', 'hereafter', 'hers', 'can', 'bill', 'here', 'too', 'were', 'himself', 'eg', 'them', 'to', 'thus', 'my', 'became', 'some', 'since', 'mill', 'someone', 'fill', 'down', 'former', 'still', 'me', 'twelve', 'so', 'there'})\n",
        "Length of feature vector: 181110\n"
       ]
      }
     ],
     "prompt_number": 72
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from sklearn.svm import LinearSVC\n",
      "\n",
      "bigram_linsvm = LinearSVC()\n",
      "predict_y = classify(bigram_linsvm)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 73
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "bigram_presence_linsvm_accuracy = accuracy()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Accuracy: 70.89999999999999%\n",
        "             precision    recall  f1-score   support\n",
        "\n",
        "        neg       0.67      0.82      0.74       500\n",
        "        pos       0.77      0.60      0.67       500\n",
        "\n",
        "avg / total       0.72      0.71      0.71      1000\n",
        "\n"
       ]
      }
     ],
     "prompt_number": 74
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "This is a major reduction in accuracy of the order of 10% (80.2% vs. 70.9%). This suggests that stopwords are vital for bigram feature vectors."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "####Bigram Counts with Naive Bayes"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Setting up the vectorizer:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Create the vectorizer\n",
      "bigram_count_vectorizer = CountVectorizer(ngram_range=(2,2), stop_words='english')\n",
      "\n",
      "# Fit training data and transform appropriately\n",
      "train_X = bigram_count_vectorizer.fit_transform(train_set.data)\n",
      "train_y = train_set.target\n",
      "\n",
      "# Transform test data according to learnt training vocabulary\n",
      "test_X = bigram_count_vectorizer.transform(test_set.data)\n",
      "test_y = test_set.target"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 75
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from sklearn.naive_bayes import MultinomialNB\n",
      "\n",
      "bigram_nb = MultinomialNB()\n",
      "predict_y = classify(bigram_nb)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 78
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "bigram_counts_nb_accuracy = accuracy()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Accuracy: 71.89999999999999%\n",
        "             precision    recall  f1-score   support\n",
        "\n",
        "        neg       0.68      0.84      0.75       500\n",
        "        pos       0.79      0.59      0.68       500\n",
        "\n",
        "avg / total       0.73      0.72      0.71      1000\n",
        "\n"
       ]
      }
     ],
     "prompt_number": 79
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The reduction for counts is also on the order of 10% (81.2% vs. 71.9%). Thus we conclude that stopwords are extremely important for bigram features and should *not* be removed."
     ]
    }
   ],
   "metadata": {}
  }
 ]
}